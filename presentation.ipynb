{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop\n",
    "\n",
    "Introduction to Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI key from env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "azure_version = \"2024-06-01\"\n",
    "azure_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "azure_embeddings = os.getenv(\"AZURE_OPENAI_EMBEDDINGS\")\n",
    "azure_whisper = os.getenv(\"AZURE_OPENAI_WHISPER\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_key = os.getenv(\"AZURE_OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create llm instance\n",
    "\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=azure_key,\n",
    "    api_version=azure_version,\n",
    "    azure_deployment=azure_deployment,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke llm\n",
    "\n",
    "llm.invoke(\"What do you know about Conferences?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke llm\n",
    "\n",
    "answer = llm.invoke(\"What do you know about Conferences?\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use prompts\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"You are an encyclopedia.\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "answer = chain.invoke({\"input\": \"What are Conferences?\"})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusue prompts\n",
    "\n",
    "answer = chain.invoke({\"input\": \"What are Conferences also called?\"})\n",
    "print(answer.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contexts\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "answer = chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Conferences also called?\",\n",
    "        \"context\": \"An academic conference or scientific conference (also congress, symposium, workshop, or meeting) is an event for researchers (not necessarily academics) to present and discuss their scholarly work. Together with academic or scientific journals and preprint archives, conferences provide an important channel for exchange of information between researchers. Further benefits of participating in academic conferences include learning effects in terms of presentation skills and 'academic habitus', receiving feedback from peers for one's own research, the possibility to engage in informal communication with peers about work opportunities and collaborations, and getting an overview of current research in one or more disciplines.\",\n",
    "    }\n",
    ")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"An academic conference or scientific conference (also congress, symposium, workshop, or meeting) is an event for researchers (not necessarily academics) to present and discuss their scholarly work. Together with academic or scientific journals and preprint archives, conferences provide an important channel for exchange of information between researchers. Further benefits of participating in academic conferences include learning effects in terms of presentation skills and 'academic habitus', receiving feedback from peers for one's own research, the possibility to engage in informal communication with peers about work opportunities and collaborations, and getting an overview of current research in one or more disciplines.\",\n",
    "        metadata={\n",
    "            \"source\": \"wikipedia\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Conferences also called?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Conferences also called and what is the source?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document prompts\n",
    "\n",
    "document_prompt = ChatPromptTemplate.from_template(\"\"\"Content: {page_content}                             \n",
    "Source: {source}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "answer = document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Conferences also called and what is the source?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector stores\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "# load data\n",
    "loader = TextLoader(r\"./Data/Internal/budget.txt\", encoding=\"utf-8\")\n",
    "data = loader.load()\n",
    "loader = TextLoader(r\"./Data/Internal/lostpassword.txt\", encoding=\"utf-8\")\n",
    "data.extend(loader.load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=30000, chunk_overlap=1000, separators=[\".\", \"\\n\"])\n",
    "documents = splitter.split_documents(data)\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    api_key=azure_key,\n",
    "    api_version=azure_version,\n",
    "    azure_deployment=azure_embeddings,\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")\n",
    "db = Chroma.from_documents(documents, embeddings, persist_directory=\"./chroma/presentation\")\n",
    "\n",
    "for document in documents:\n",
    "    print(document)\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever\n",
    "\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "db = Chroma(persist_directory=\"./chroma/presentation\", embedding_function=embeddings)\n",
    "retriever = db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "answer = retrieval_chain.invoke({\"input\": \"What is the title of step two when losing a password?\"})\n",
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retrieval_chain.invoke({\"input\": \"Give a concise description how I can apply for a budget.\"})\n",
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = ChatPromptTemplate.from_template(\"\"\"Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "Text: {context}\"\"\")\n",
    "\n",
    "summary_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=summary_prompt,\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "answer = summary_chain.invoke(\n",
    "    {\"context\": documents}\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "\n",
    "# from langchain.chains.combine_documents import collapse_docs, split_list_of_docs\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.schema import StrOutputParser\n",
    "# from langchain_core.prompts import format_document\n",
    "# from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# partial_format_document = partial(format_document, prompt=document_prompt)\n",
    "\n",
    "# # The chain we'll apply to each individual document.\n",
    "# # Returns a summary of the document.\n",
    "# map_chain = (\n",
    "#     {\"context\": partial_format_document}\n",
    "#     | summary_prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# # A wrapper chain to keep the original Document metadata\n",
    "# map_as_doc_chain = (\n",
    "#     RunnableParallel({\"doc\": RunnablePassthrough(), \"content\": map_chain})\n",
    "#     | (lambda x: Document(page_content=x[\"content\"], metadata=x[\"doc\"].metadata))\n",
    "# ).with_config(run_name=\"Summarize (return doc)\")\n",
    "\n",
    "# # The chain we'll repeatedly apply to collapse subsets of the documents\n",
    "# # into a consolidate document until the total token size of our\n",
    "# # documents is below some max size.\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(partial_format_document(doc) for doc in docs)\n",
    "\n",
    "# collapse_chain = (\n",
    "#     {\"context\": format_docs}\n",
    "#     | PromptTemplate.from_template(\"Collapse this content:\\n\\n{context}\")\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "# def get_num_tokens(docs):\n",
    "#     return llm.get_num_tokens(format_docs(docs))\n",
    "\n",
    "# def collapse(\n",
    "#     docs,\n",
    "#     config,\n",
    "#     token_max=4000,\n",
    "# ):\n",
    "#     collapse_ct = 1\n",
    "#     while get_num_tokens(docs) > token_max:\n",
    "#         config[\"run_name\"] = f\"Collapse {collapse_ct}\"\n",
    "#         invoke = partial(collapse_chain.invoke, config=config)\n",
    "#         split_docs = split_list_of_docs(docs, get_num_tokens, token_max)\n",
    "#         docs = [collapse_docs(_docs, invoke) for _docs in split_docs]\n",
    "#         collapse_ct += 1\n",
    "#     return docs\n",
    "\n",
    "# # The chain we'll use to combine our individual document summaries\n",
    "# # (or summaries over subset of documents if we had to collapse the map results)\n",
    "# # into a final summary.\n",
    "\n",
    "# reduce_chain = (\n",
    "#     {\"context\": format_docs}\n",
    "#     | PromptTemplate.from_template(\"Combine these summaries:\\n\\n{context}\")\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# ).with_config(run_name=\"Reduce\")\n",
    "\n",
    "# # The final full chain\n",
    "# map_reduce = (map_as_doc_chain.map() | collapse | reduce_chain).with_config(run_name=\"Map reduce\")\n",
    "\n",
    "# map_reduce.invoke(\n",
    "#     input=documents,\n",
    "#     config={\"max_concurrency\": 5},\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\n",
    "Now tell me which intent the following question has.\n",
    "                                            \n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "answer = intent_chain.invoke({\"input\": \"Let's go have some food at Wendy's.\"})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\n",
    "Now tell me which intent the following question has. Only answer with two/three words.\n",
    "                                            \n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "answer = intent_chain.invoke({\"input\": \"Let's go have some food at Wendy's.\"})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Booking a flight, Getting a reservation or ordering food.\n",
    "\n",
    "Consider the following examples:\n",
    "\n",
    "Question: Let's go to Tokyo next week.\n",
    "Answer: Booking a flight\n",
    "\n",
    "Question: Get some pizza delivered for this evening.\n",
    "Answer: Ordering food\n",
    "\n",
    "Question: How about a movie at 8pm at the cinema?\n",
    "Anmswer: Getting a reservation\n",
    "\n",
    "Now tell me which intent the follolwing question has. Only answer with two/three words.\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "answer = intent_chain.invoke({\"input\": \"Let's go have some food at Wendy's.\"})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm.invoke(\"What did Frank-Walter Steinmeier say in his christmas speech 2023?\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.tools.ddg_search import DuckDuckGoSearchRun\n",
    "\n",
    "ddg = DuckDuckGoSearchRun()\n",
    "\n",
    "ddg_tool = Tool.from_function(\n",
    "    func = ddg.run,\n",
    "    name = \"DuckDuckGo Search\",\n",
    "    description = \"Search DuckDuckGo for a query about current events.\",\n",
    ")\n",
    "\n",
    "tools = [ddg_tool]\n",
    "\n",
    "ddg_tool.run(\"What did Frank-Walter Steinmeier say in his christmas speech 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "agent_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a great AI-Assistant. You have access to tools to help you answer questions:\n",
    "\n",
    "{tools}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "\n",
    "'''\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat 3 times)\n",
    "'''\n",
    "\n",
    "When you have a response to say or question to ask to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "'''\n",
    "Final Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response or question here]\n",
    "'''\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\")\n",
    "agent = create_react_agent(\n",
    "    tools=tools, llm=llm, prompt=agent_prompt\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, max_iterations=30, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What did Frank-Walter Steinmeier say in his christmas speech 2023?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_agent_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a great AI-Assistant and only answer in 10 words. You have access to tools to help you answer questions:\n",
    "\n",
    "{tools}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "\n",
    "'''\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat 3 times)\n",
    "'''\n",
    "\n",
    "When you have a response to say or question to ask to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "'''\n",
    "Final Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response or question here]\n",
    "'''\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "\"\"\")\n",
    "agent = create_react_agent(\n",
    "    tools=tools, llm=llm, prompt=new_agent_prompt\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, max_iterations=30, verbose=True)\n",
    "\n",
    "\n",
    "agent_executor.invoke({\"input\": \"What did Frank-Walter Steinmeier say in his christmas speech 2023?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "simple_summary_prompt = ChatPromptTemplate.from_template(\"\"\"Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand. Keep it funky!\n",
    "\n",
    "Text: {input}\"\"\")\n",
    "simple_summary_chain = {\"input\": RunnablePassthrough()} | simple_summary_prompt | llm\n",
    "\n",
    "summary_tool = Tool(\n",
    "    name=\"Funky Summary Tool\",\n",
    "    func=simple_summary_chain.invoke,\n",
    "    description=\"Use this tool to do a funky summary. Make sure you get the text to do a summary of first.\"\n",
    ")\n",
    "tools.append(summary_tool)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    tools=tools, llm=llm, prompt=agent_prompt\n",
    ")\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, max_iterations=30, verbose=True)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"Funky summarize the christmas speech 2023 of Frank-Walter Steinmeier?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm.invoke(\"What is 1000 + 1234?\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm.invoke(\"What is 13 raised to the .3432 power?\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from mimetypes import guess_type\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "\n",
    "# Function to encode a local image into data URL \n",
    "def local_image_to_data_url(image_path):\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    # Default to png\n",
    "    if mime_type is None:\n",
    "        mime_type = 'image/png'\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n",
    "\n",
    "\n",
    "prompt_template =  HumanMessagePromptTemplate.from_template(\n",
    "            template=[\n",
    "                {\"type\": \"text\", \"text\": \"Summarize this image\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": \"{encoded_image_url}\",\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "summarize_image_prompt = ChatPromptTemplate.from_messages([prompt_template])\n",
    "\n",
    "gpt4_image_chain = summarize_image_prompt | llm \n",
    "\n",
    "img_file = \"Images/UserGuide.jpg\"\n",
    "page3_encoded = local_image_to_data_url(img_file)\n",
    "\n",
    "answer = gpt4_image_chain.invoke(input={\"encoded_image_url\":page3_encoded})\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    api_key=azure_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=azure_whisper,\n",
    "    api_version=azure_version,\n",
    ")\n",
    "\n",
    "def get_transcript(audio_file):\n",
    "    if not os.path.exists(audio_file):\n",
    "        audio_file = \"./Audio/\" + audio_file\n",
    "    client.audio.with_raw_response\n",
    "    return client.audio.transcriptions.create(\n",
    "        file=open(audio_file, \"rb\"),            \n",
    "        model=\"whisper\",\n",
    "        language=\"de\",\n",
    "    ).text\n",
    "\n",
    "audio_test_file = \"./Audio/newyear2023.mp3\"\n",
    "print(get_transcript(audio_test_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio\n",
    "def get_audio_doc(input):\n",
    "    return {\n",
    "        \"context\": [Document(\n",
    "            page_content=get_transcript(input),\n",
    "            metadata={\n",
    "                \"source\": \"audio\"\n",
    "            }\n",
    "        )]\n",
    "    }\n",
    "audio_summary_chain = get_audio_doc | summary_chain\n",
    "answer = audio_summary_chain.invoke(\"./Audio/newyear2023.mp3\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
